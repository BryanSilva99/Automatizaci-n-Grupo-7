# -*- coding: utf-8 -*-
"""GRUPO 7 - CLUSTERIZACI칍N DE REPOSITORIOS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KM_zsJeB5mUox2PzUgRQAPkyf820ngeD
"""

# DESCRIPCI칍N DEL PROYECTO
# Este proyecto est치 dise침ado para analizar y clusterizar repositorios de GitHub mediante un enfoque h칤brido
# que combina an치lisis est치tico de c칩digo con inteligencia artificial. El flujo general del proyecto es el siguiente:
#
# 1. Clona repositorios de c칩digo desde GitHub en directorios temporales.
# 2. Extrae caracter칤sticas tecnol칩gicas relevantes de archivos README, YAML, Dockerfile y docker-compose.yml,
#    identificando tecnolog칤as como Docker, Kubernetes, bases de datos (PostgreSQL, MySQL, Redis), frameworks
#    (React, FastAPI, Angular) y lenguajes de programaci칩n.
# 3. Analiza espec칤ficamente los archivos docker-compose.yml para mapear la topolog칤a de microservicios y sus
#    dependencias inter-servicios.
# 4. Visualiza estas topolog칤as mediante grafos dirigidos usando NetworkX y Matplotlib, mostrando las relaciones
#    y dependencias entre componentes.
# 5. Integra la API de Groq (usando el modelo Llama) para generar explicaciones autom치ticas sobre las arquitecturas
#    detectadas y justificar las agrupaciones de repositorios.
# 6. Implementa un algoritmo gen칠tico personalizado que utiliza similitud de coseno para clusterizar los repositorios
#    en grupos basados en sus caracter칤sticas tecnol칩gicas compartidas, permitiendo identificar patrones arquitect칩nicos
#    y tecnol칩gicos comunes entre proyectos similares.

# !pip install gitpython pyyaml scikit-learn deap networkx matplotlib
# !pip install deap pyvis scikit-learn

from dotenv import load_dotenv
import stat
import shutil
from sklearn.metrics.pairwise import cosine_similarity
import random
import numpy as np
from matplotlib.patches import Patch
from collections import defaultdict
import matplotlib.pyplot as plt
import os
import yaml
import git
import tempfile
import requests
import networkx as nx
import matplotlib
matplotlib.use('Agg')  # 游댠 Fuerza matplotlib a usar un backend sin GUI


# === Configuraci칩n de la API de Groq ===

load_dotenv()

groq_api_key = os.getenv("GROQ_API_KEY")
groq_url = os.getenv("GROQ_API_URL")

# Funci칩n que obtiene una respuesta de la API de Groq basada en un mensaje dado.


def obtener_respuesta_groq(mensaje):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    payload = {
        "model": "meta-llama/llama-4-scout-17b-16e-instruct",
        "messages": [{
            "role": "user",
            "content": mensaje
        }]
    }
    try:
        response = requests.post(GROQ_API_URL, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        return data.get("choices", [{}])[0].get("message", {}).get("content", "No se obtuvo respuesta.")
    except requests.exceptions.RequestException as e:
        print(f"Error al hacer la solicitud a la API de Groq: {e}")
        return None


# Tecnolog칤as que buscas en el repositorio
FEATURES = ["Docker", "Kubernetes", "CI/CD", "Helm", "Terraform", "Python", "Node.js", "Java", "Ruby", "Go",
            "PostgreSQL", "MySQL", "Redis", "MongoDB", "Nginx", "Apache", "React", "Vue", "Angular", "FastAPI"]


def clone_repo(repo_url):
    """Clona un repositorio en un directorio temporal"""
    try:
        temp_dir = tempfile.mkdtemp()
        print(f"Clonando {repo_url} en {temp_dir}")
        git.Repo.clone_from(repo_url, temp_dir)
        return temp_dir
    except Exception as e:
        print(f"Error al clonar {repo_url}: {e}")
        return None


def handle_remove_readonly(func, path, exc_info):
    import os
    os.chmod(path, stat.S_IWRITE)
    func(path)


def cleanup_repo(path):
    try:
        shutil.rmtree(path, onerror=handle_remove_readonly)
    except Exception as e:
        print("丘멆잺 Error al eliminar el repositorio clonado:", e)


def parse_yaml_files(repo_path):
    """Analiza archivos YAML en el repositorio, manejando m칰ltiples documentos."""
    yaml_files = []

    # Buscar archivos .yaml y .yml
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.endswith(".yaml") or file.endswith(".yml"):
                file_path = os.path.join(root, file)
                yaml_files.append(file_path)

    # Parsear cada archivo YAML encontrado
    all_yaml_docs = []
    yaml_content = ""

    for yaml_file in yaml_files:
        try:
            with open(yaml_file, 'r', encoding='utf-8', errors='ignore') as file:
                content = file.read()
                yaml_content += content + "\n"

                # Permitir m칰ltiples documentos en un solo archivo YAML
                try:
                    documents = list(yaml.safe_load_all(content))
                    all_yaml_docs.extend(
                        [doc for doc in documents if doc is not None])
                except yaml.YAMLError:
                    # Si falla el parsing YAML, al menos tenemos el contenido como texto
                    pass
        except Exception as e:
            print(f"Error al leer el archivo YAML {yaml_file}: {e}")

    return all_yaml_docs, yaml_content


def parse_docker_compose_or_yaml(repo_path):
    """Analiza docker-compose.yml o cualquier archivo .yaml en busca de dependencias"""
    docker_compose_files = []
    docker_compose_dependencies = []

    # Buscar archivos docker-compose.yml y otros YAMLs
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if "docker-compose" in file.lower() and (file.endswith(".yml") or file.endswith(".yaml")):
                docker_compose_files.append(os.path.join(root, file))

    # Analizar todos los archivos docker-compose
    for docker_compose_file in docker_compose_files:
        try:
            with open(docker_compose_file, 'r', encoding='utf-8', errors='ignore') as file:
                content = file.read()

                # Permitir m칰ltiples documentos en un solo archivo YAML
                try:
                    documents = list(yaml.safe_load_all(content))

                    for doc in documents:
                        if doc and "services" in doc:
                            for service, data in doc["services"].items():
                                dependencies = []

                                # Buscar diferentes tipos de dependencias
                                if "depends_on" in data:
                                    if isinstance(data["depends_on"], list):
                                        dependencies.extend(data["depends_on"])
                                    elif isinstance(data["depends_on"], dict):
                                        dependencies.extend(
                                            data["depends_on"].keys())

                                # Buscar links
                                if "links" in data:
                                    dependencies.extend(data["links"])

                                docker_compose_dependencies.append({
                                    "name": service,
                                    "dependencies": dependencies,
                                    "image": data.get("image", ""),
                                    "ports": data.get("ports", []),
                                    "environment": data.get("environment", {})
                                })
                except yaml.YAMLError as e:
                    print(
                        f"Error al parsear YAML en {docker_compose_file}: {e}")

        except Exception as e:
            print(f"Error al analizar el archivo {docker_compose_file}: {e}")

    return docker_compose_dependencies


def extract_readme_or_yaml(repo_path):
    """Extrae el contenido del README.md y de archivos YAML para an치lisis"""
    content = ""

    # Extraer README
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.lower() in ["readme.md", "readme.txt", "readme.rst"]:
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:
                        content += f.read() + "\n"
                except Exception as e:
                    print(f"Error al leer {file}: {e}")

    # Extraer contenido de archivos YAML
    _, yaml_content = parse_yaml_files(repo_path)
    content += yaml_content

    # Extraer contenido de Dockerfile
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.lower() == "dockerfile" or file.lower().startswith("dockerfile"):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:
                        content += f.read() + "\n"
                except Exception as e:
                    print(f"Error al leer {file}: {e}")

    return content


def extract_technologies_from_text(text):
    """Extrae las tecnolog칤as del texto y de estructuras YAML"""
    technologies = set()
    text_lower = text.lower()

    # Buscar tecnolog칤as por palabras clave
    for feature in FEATURES:
        if feature.lower() in text_lower:
            technologies.add(feature)

    # Buscar patrones espec칤ficos
    patterns = {
        "Docker": ["dockerfile", "docker-compose", "FROM ", "RUN ", "COPY ", "ADD "],
        "Kubernetes": ["apiVersion:", "kind:", "metadata:", "spec:", "kubectl"],
        "PostgreSQL": ["postgres", "postgresql", "psql"],
        "MySQL": ["mysql", "mariadb"],
        "Redis": ["redis"],
        "MongoDB": ["mongo", "mongodb"],
        "Nginx": ["nginx"],
        "Apache": ["apache", "httpd"],
        "Python": [".py", "python", "pip", "requirements.txt", "setup.py"],
        "Node.js": ["package.json", "npm", "node", "yarn"],
        "Java": [".java", "maven", "gradle", "pom.xml"],
        "Go": [".go", "go.mod", "go.sum"],
        "React": ["react", "jsx"],
        "Vue": ["vue"],
        "Angular": ["angular", "@angular"],
        "FastAPI": ["fastapi", "uvicorn"]
    }

    for tech, keywords in patterns.items():
        for keyword in keywords:
            if keyword in text_lower:
                technologies.add(tech)
                break

    return list(technologies)


def generate_topology_explanation(topology):
    """Genera una explicaci칩n breve de la topolog칤a usando Groq"""
    if not topology:
        return "No se encontraron componentes para analizar."

    services = [s['name'] for s in topology]
    dependencies_info = []

    for service in topology:
        if service.get('dependencies'):
            deps = ', '.join(service['dependencies'])
            dependencies_info.append(f"{service['name']} depende de: {deps}")

    mensaje = f"""Analiza brevemente esta arquitectura de microservicios en m치ximo 3 l칤neas (no menciones que es en 3 l칤neas):
Servicios: {', '.join(services)}
Dependencias: {'; '.join(dependencies_info) if dependencies_info else 'Sin dependencias expl칤citas'}
Describe la relaci칩n entre componentes de forma concisa."""

    explicacion = obtener_respuesta_groq(mensaje)
    return explicacion if explicacion else "Arquitectura con servicios interconectados seg칰n dependencias definidas."


def draw_topology_interactive(topology, title="Mapa Topol칩gico", repo_url="", image_name="topo.png"):
    if not topology:
        print(f"No se encontr칩 topolog칤a para {repo_url}")
        return None

    G = nx.DiGraph()
    for service in topology:
        G.add_node(service["name"])
        for dep in service.get("dependencies", []):
            G.add_edge(service["name"], dep)

    if len(G.nodes()) == 0:
        print(f"No se encontraron servicios en {repo_url}")
        return None

    # Crear grafo
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, k=1, iterations=50)
    nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                           node_size=3000, alpha=0.7)
    nx.draw_networkx_edges(G, pos, edge_color='gray',
                           arrows=True, arrowsize=20, alpha=0.6)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')
    plt.title(f"{title}\n{repo_url}")
    plt.axis('off')
    plt.tight_layout()

    # Guardar como imagen en la carpeta static
    output_path = os.path.join("app", "static", image_name)
    plt.savefig(output_path)
    plt.close()

    return f"/static/{image_name}"


def genetic_algorithm(features, num_clusters=3):
    """Algoritmo gen칠tico simple para agrupaci칩n"""
    if not features:
        return []

    population_size = 50
    generations = 100
    mutation_rate = 0.1

    # Inicializar poblaci칩n
    population = []
    for _ in range(population_size):
        individual = [random.randint(0, num_clusters - 1)
                      for _ in range(len(features))]
        population.append(individual)

    def fitness(individual):
        # Calcular fitness basado en cohesi칩n intra-cluster y separaci칩n inter-cluster
        clusters = defaultdict(list)
        for i, cluster_id in enumerate(individual):
            clusters[cluster_id].append(features[i])

        total_fitness = 0
        for cluster_features in clusters.values():
            if len(cluster_features) > 1:
                # Calcular similitud promedio dentro del cluster
                similarities = []
                for i in range(len(cluster_features)):
                    for j in range(i + 1, len(cluster_features)):
                        sim = cosine_similarity([cluster_features[i]], [
                                                cluster_features[j]])[0][0]
                        similarities.append(sim)
                if similarities:
                    total_fitness += np.mean(similarities)

        return total_fitness

    # Evoluci칩n
    for generation in range(generations):
        # Evaluar fitness
        fitness_scores = [fitness(ind) for ind in population]

        # Selecci칩n
        sorted_population = [x for _, x in sorted(
            zip(fitness_scores, population), reverse=True)]

        # Nueva generaci칩n
        # Mantener los mejores
        new_population = sorted_population[:population_size // 2]

        # Crossover y mutaci칩n
        while len(new_population) < population_size:
            parent1 = random.choice(sorted_population[:10])
            parent2 = random.choice(sorted_population[:10])

            # Crossover
            crossover_point = random.randint(1, len(features) - 1)
            child = parent1[:crossover_point] + parent2[crossover_point:]

            # Mutaci칩n
            for i in range(len(child)):
                if random.random() < mutation_rate:
                    child[i] = random.randint(0, num_clusters - 1)

            new_population.append(child)

        population = new_population

    # Retornar el mejor individuo
    fitness_scores = [fitness(ind) for ind in population]
    best_individual = population[fitness_scores.index(max(fitness_scores))]

    return best_individual


def generate_cluster_explanation(clusters, repo_technologies):
    """Genera explicaci칩n de clusters usando Groq"""
    cluster_descriptions = []

    for cluster_id, repos in clusters.items():
        if not repos:
            continue

        # Obtener tecnolog칤as comunes del cluster
        cluster_tech = []
        for repo in repos:
            if repo in repo_technologies:
                cluster_tech.extend(repo_technologies[repo])

        # Contar frecuencias de tecnolog칤as
        tech_counts = defaultdict(int)
        for tech in cluster_tech:
            tech_counts[tech] += 1

        common_techs = [tech for tech,
                        count in tech_counts.items() if count > 1]

        mensaje = f"""Explica de manera muy breve (por ejemplo, un par de l칤neas) por qu칠 estos repositorios est치n agrupados:
Repositorios: {', '.join([r.split('/')[-1] for r in repos])}
Tecnolog칤as comunes: {', '.join(common_techs) if common_techs else 'Variadas'}
Raz칩n de agrupaci칩n:"""

        explicacion = obtener_respuesta_groq(mensaje)
        if explicacion:
            cluster_descriptions.append(f"Cl칰ster {cluster_id}: {explicacion}")
        else:
            cluster_descriptions.append(
                f"Cl칰ster {cluster_id}: Agrupados por similitud tecnol칩gica y arquitect칩nica.")

    return cluster_descriptions


def cluster_repositories_with_genetic_algorithm(repos, num_clusters=3):
    repo_features = []
    repo_explanations = []
    successful_repos = []
    repo_technologies = {}  # Para almacenar las tecnolog칤as de cada repo

    # Extraer caracter칤sticas y explicaciones
    for repo_url in repos:
        print(f"\n--- Analizando {repo_url} ---")
        repo_path = clone_repo(repo_url)

        if repo_path is None:
            print(f"Saltando {repo_url} debido a error en clonaci칩n")
            continue

        try:
            # Extraer texto de README y archivos YAML
            text = extract_readme_or_yaml(repo_path)
            technologies = extract_technologies_from_text(text)
            print(f"Tecnolog칤as encontradas: {technologies}")

            # Codificar caracter칤sticas
            encoded_features = [
                1 if f in technologies else 0 for f in FEATURES]
            repo_features.append(encoded_features)
            successful_repos.append(repo_url)
            repo_technologies[repo_url] = technologies  # Almacenar tecnolog칤as

            explanation = f"Repositorio: {repo_url} usa las tecnolog칤as: {', '.join(technologies) if technologies else 'Ninguna detectada'}"
            repo_explanations.append(explanation)

            # Generar y mostrar mapa topol칩gico
            topology = parse_docker_compose_or_yaml(repo_path)
            if topology:
                print(
                    f"Servicios encontrados: {[s['name'] for s in topology]}")
                draw_topology_interactive(
                    topology, f"Mapa Topol칩gico", repo_url)
            else:
                print("No se encontr칩 archivo .yml o servicios")

        except Exception as e:
            print(f"Error procesando {repo_url}: {e}")
        finally:
            # Limpiar directorio temporal
            cleanup_repo(repo_path)

    if not repo_features:
        print("No se pudieron procesar repositorios")
        return {}, []

    # Ejecutar algoritmo gen칠tico para agrupaci칩n
    print(
        f"\nEjecutando algoritmo gen칠tico con {len(successful_repos)} repositorios...")
    best_individual = genetic_algorithm(repo_features, num_clusters)

    # Agrupaci칩n final
    clusters = defaultdict(list)
    for repo_idx, cluster_id in enumerate(best_individual):
        clusters[cluster_id].append(successful_repos[repo_idx])

    # Generar explicaciones de clusters con Groq
    cluster_explanations = generate_cluster_explanation(
        clusters, repo_technologies)

    return clusters, cluster_explanations


# MAIN
if __name__ == "__main__":
    repos = [
        "https://github.com/microservices-demo/microservices-demo",
        "https://github.com/spring-petclinic/spring-petclinic-microservices",
        "https://github.com/kgrzybek/modular-monolith-with-ddd",
        "https://github.com/tiangolo/full-stack-fastapi-postgresql",
        "https://github.com/react-boilerplate/react-boilerplate",
        "https://github.com/kelseyhightower/kubernetes-the-hard-way",
        "https://github.com/eventuate-tram/eventuate-tram-examples-customers-and-orders",
        "https://github.com/didtheyghostme/didtheyghostme",
        "https://github.com/palinkiewicz/lyricpost",
        "https://github.com/petrousoft/kubejyg"
    ]

    print("Iniciando an치lisis de repositorios...")
    clusters, explanations = cluster_repositories_with_genetic_algorithm(
        repos, num_clusters=3)

    print("\n=== RESULTADOS DE CLUSTERING ===")
    for i, cluster in clusters.items():
        print(f"\nCl칰ster {i}:")
        for repo in cluster:
            print(f"  - {repo}")

    print("\n=== EXPLICACIONES DE AGRUPACI칍N ===")
    for explanation in explanations:
        print(f"游댌 {explanation}")
